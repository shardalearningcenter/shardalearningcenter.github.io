<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How to train LLM using local dataset | ShardaLearningCenter</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="How to train LLM using local dataset" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to train LLM using local dataset" />
<meta property="og:description" content="How to train LLM using local dataset" />
<link rel="canonical" href="https://shaardalearningcenter.github.io/2025/08/29/train-llm-using-local-dataset.html" />
<meta property="og:url" content="https://shaardalearningcenter.github.io/2025/08/29/train-llm-using-local-dataset.html" />
<meta property="og:site_name" content="ShardaLearningCenter" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-29T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How to train LLM using local dataset" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-08-29T00:00:00+05:30","datePublished":"2025-08-29T00:00:00+05:30","description":"How to train LLM using local dataset","headline":"How to train LLM using local dataset","mainEntityOfPage":{"@type":"WebPage","@id":"https://shaardalearningcenter.github.io/2025/08/29/train-llm-using-local-dataset.html"},"url":"https://shaardalearningcenter.github.io/2025/08/29/train-llm-using-local-dataset.html"}</script>
<!-- End Jekyll SEO tag -->


    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://shaardalearningcenter.github.io/">
          <h1>ShardaLearningCenter</h1>
        </a>
        <h2>Learn to code by building real projects. Fast. Practical. No fluff.</h2>
        
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How to train LLM using local dataset | ShardaLearningCenter</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="/assets/css/style.css">
  <style>
    
    .post-container {
      max-width: 1400px;
      margin: 2rem auto;
      padding: 2rem;
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }
    .post-title {
      font-size: 2.2rem;
      margin-bottom: 0.5rem;
      color: #222;
    }
    .post-meta {
      font-size: 0.9rem;
      color: #666;
      margin-bottom: 1.5rem;
    }
    .post-content {
      font-size: 1.05rem;
      line-height: 1.7;
      color: #333;
    }
    .post-content h2, .post-content h3 {
      margin-top: 1.8rem;
      margin-bottom: 0.8rem;
      font-weight: 600;
      color: #111;
    }
    .post-content p {
      margin-bottom: 1.2rem;
    }
    .post-content a {
      color: #0066cc;
      text-decoration: underline;
    }
    .post-content blockquote {
      border-left: 4px solid #0066cc;
      padding-left: 1rem;
      font-style: italic;
      color: #555;
      background: #f9f9f9;
      margin: 1.5rem 0;
    }
    .post-content code {
      background: #f4f4f4;
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-size: 0.95rem;
    }
    .post-content pre {
      background: #272822;
      color: #f8f8f2;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <nav>
    <h2>Sharda Learning Center</h2>
    <ul>
      <li><a href="#home">Home</a></li>
      <li><a href="#about">About</a></li>
      <li><a href="#courses">Courses</a></li>
      <li><a href="/blog">Blog</a></li>
      <li><a href="#contact">Contact</a></li>
      <li><a href="#socials">Socials</a></li>
    </ul>
  </nav>
  <main class="post-container">
    <h1 class="post-title">How to train LLM using local dataset</h1>
    <div class="post-meta">
      August 29, 2025
    </div>
    <div class="post-content">
      <h1 id="how-to-train-llm-using-local-dataset">How to train LLM using local dataset</h1>

<p>Clone the required repo for training</p>
<ul>
  <li>git clone https://github.com/huggingface/transformers</li>
  <li>cd transformers</li>
  <li>pip install .</li>
  <li>cd /mnt/d/karm/slc-bootcamp/</li>
  <li>cd training-exmple/</li>
  <li>cd transformers/</li>
  <li>pip install .</li>
  <li>python run_summarization.py</li>
  <li>pip install datasets</li>
  <li>python run_summarization.py</li>
  <li>cd my-example/</li>
  <li>pip install -r requirements.txt</li>
</ul>

<h2 id="create-requirementstxt">create requirements.txt</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transformers</span><span class="o">&gt;=</span><span class="mf">4.44</span><span class="p">.</span><span class="mi">0</span>
<span class="n">datasets</span><span class="o">&gt;=</span><span class="mf">2.20</span><span class="p">.</span><span class="mi">0</span>
<span class="n">evaluate</span><span class="o">&gt;=</span><span class="mf">0.4</span><span class="p">.</span><span class="mi">2</span>
<span class="n">torch</span><span class="o">&gt;=</span><span class="mf">2.3</span><span class="p">.</span><span class="mi">0</span>
<span class="n">accelerate</span><span class="o">&gt;=</span><span class="mf">0.33</span><span class="p">.</span><span class="mi">0</span>
<span class="n">sentencepiece</span><span class="o">&gt;=</span><span class="mf">0.2</span><span class="p">.</span><span class="mi">0</span>

</code></pre></div></div>
<h2 id="take-this-code-snipette-to-train-the-llm-using-the-cnn-data-set">Take this code snipette to train the llm using the cnn data set</h2>
<ul>
  <li>create file run_summarization.py</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">Seq2SeqTrainer</span><span class="p">,</span> <span class="n">Seq2SeqTrainingArguments</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">DataCollatorForSeq2Seq</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># 1. Load dataset
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">cnn_dailymail</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">3.0.0</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 2. Load tokenizer and model
# model_name = "google-t5/t5-small"
</span><span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">google/flan-t5-base</span><span class="sh">"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Preprocess function
</span><span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">summarize: </span><span class="sh">"</span> <span class="o">+</span> <span class="n">doc</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="sh">"</span><span class="s">article</span><span class="sh">"</span><span class="p">]]</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="sh">"</span><span class="s">highlights</span><span class="sh">"</span><span class="p">],</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">model_inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">labels</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">model_inputs</span>

<span class="c1"># Tokenize dataset
</span><span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">preprocess_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">article</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">highlights</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># 3. Data collator
</span><span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorForSeq2Seq</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># 4. Training arguments (LOCAL SAVE ONLY)
</span><span class="n">training_args</span> <span class="o">=</span> <span class="nc">Seq2SeqTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./local_t5_summarization</span><span class="sh">"</span><span class="p">,</span>   <span class="c1"># local dir
</span>    <span class="n">eval_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>
    <span class="c1"># evaluate_during_training=True,  # old name
</span>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
<span class="c1"># max_steps=2000,  # overrides num_train_epochs
</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">predict_with_generate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">logging_dir</span><span class="o">=</span><span class="sh">"</span><span class="s">./logs</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">report_to</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span>  <span class="c1"># disables wandb/hub
</span><span class="p">)</span>

<span class="c1"># 5. Trainer
</span><span class="n">trainer</span> <span class="o">=</span> <span class="nc">Seq2SeqTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)),</span>  <span class="c1"># small subset for local test
</span>    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="sh">"</span><span class="s">validation</span><span class="sh">"</span><span class="p">].</span><span class="nf">select</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)),</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 6. Train
</span><span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="c1"># 7. Save locally
</span><span class="n">trainer</span><span class="p">.</span><span class="nf">save_model</span><span class="p">(</span><span class="sh">"</span><span class="s">./local_t5_summarization</span><span class="sh">"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">save_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">./local_t5_summarization</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✅ Model saved locally at ./local_t5_summarization</span><span class="sh">"</span><span class="p">)</span>


</code></pre></div></div>

<h2 id="use-this-trained-model">Use this trained model</h2>
<p>This model is saved at local_t5_summarization load it</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1"># Path where your trained model was saved
# model_path = "./local_t5_summarization/checkpoint-250"
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./local_t5_summarization/checkpoint-250</span><span class="sh">"</span>  <span class="c1"># your trained checkpoint
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Prime Minister Narendra Modi on Saturday (August 30, 2025) travelled to Sendai in the Japanese prefecture of Miyagi to visit a semiconductor plant. Narendra Modi on Saturday (August 30, 2025) also met governors of 16 Japanese prefectures in Tokyo and called for strengthening state-prefecture cooperation under the India-Japan Special Strategic and Global Partnership, the Ministry of External Affairs (MEA) said in a statement.</span><span class="sh">"""</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">summarize: </span><span class="sh">"</span> <span class="o">+</span> <span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">summary_ids</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">min_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">length_penalty</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">early_stopping</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Original</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Summary:</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">summary_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>


</code></pre></div></div>
<h1 id="original">Original</h1>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prime Minister Narendra Modi on Saturday (August 30, 2025) travelled to Sendai in the Japanese prefecture of Miyagi to visit a semiconductor plant. Narendra Modi on Saturday (August 30, 2025) also met governors of 16 Japanese prefectures in Tokyo and called for strengthening state-prefecture cooperation under the India-Japan Special Strategic and Global Partnership, the Ministry of External Affairs (MEA) said in a statement.
</code></pre></div></div>
<h1 id="summary">Summary</h1>
<p>```
Summary: PM Narendra Modi visits a semiconductor plant in Miyagi . He also meets governors of 16 Japanese prefectures in Tokyo . PM calls for strengthening state-prefecture cooperation under India-Japan Special Strategic</p>


    </div>
  </main>

</body>
</html>
        </section>

        <aside id="sidebar">
          

          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>
        </aside>
      </div>
    </div>

  </body>
</html>
