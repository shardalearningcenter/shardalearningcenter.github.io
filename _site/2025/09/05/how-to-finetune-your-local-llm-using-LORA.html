<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How to finetune your local llm using LORA | ShardaLearningCenter</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="How to finetune your local llm using LORA" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Train in hours not days using LORA LoRA = Tiny Add-On Brains üß†" />
<meta property="og:description" content="Train in hours not days using LORA LoRA = Tiny Add-On Brains üß†" />
<link rel="canonical" href="https://shaardalearningcenter.github.io/2025/09/05/how-to-finetune-your-local-llm-using-LORA.html" />
<meta property="og:url" content="https://shaardalearningcenter.github.io/2025/09/05/how-to-finetune-your-local-llm-using-LORA.html" />
<meta property="og:site_name" content="ShardaLearningCenter" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-05T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How to finetune your local llm using LORA" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-05T00:00:00+05:30","datePublished":"2025-09-05T00:00:00+05:30","description":"Train in hours not days using LORA LoRA = Tiny Add-On Brains üß†","headline":"How to finetune your local llm using LORA","mainEntityOfPage":{"@type":"WebPage","@id":"https://shaardalearningcenter.github.io/2025/09/05/how-to-finetune-your-local-llm-using-LORA.html"},"url":"https://shaardalearningcenter.github.io/2025/09/05/how-to-finetune-your-local-llm-using-LORA.html"}</script>
<!-- End Jekyll SEO tag -->


    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://shaardalearningcenter.github.io/">
          <h1>ShardaLearningCenter</h1>
        </a>
        <h2>Learn to code by building real projects. Fast. Practical. No fluff.</h2>
        
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How to finetune your local llm using LORA | ShardaLearningCenter</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="/assets/css/style.css">
  <style>
    
    .post-container {
      max-width: 1400px;
      margin: 2rem auto;
      padding: 2rem;
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }
    .post-title {
      font-size: 2.2rem;
      margin-bottom: 0.5rem;
      color: #222;
    }
    .post-meta {
      font-size: 0.9rem;
      color: #666;
      margin-bottom: 1.5rem;
    }
    .post-content {
      font-size: 1.05rem;
      line-height: 1.7;
      color: #333;
    }
    .post-content h2, .post-content h3 {
      margin-top: 1.8rem;
      margin-bottom: 0.8rem;
      font-weight: 600;
      color: #111;
    }
    .post-content p {
      margin-bottom: 1.2rem;
    }
    .post-content a {
      color: #0066cc;
      text-decoration: underline;
    }
    .post-content blockquote {
      border-left: 4px solid #0066cc;
      padding-left: 1rem;
      font-style: italic;
      color: #555;
      background: #f9f9f9;
      margin: 1.5rem 0;
    }
    .post-content code {
      background: #f4f4f4;
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-size: 0.95rem;
    }
    .post-content pre {
      background: #272822;
      color: #f8f8f2;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <nav>
    <h2>Sharda Learning Center</h2>
    <ul>
      <li><a href="#home">Home</a></li>
      <li><a href="#about">About</a></li>
      <li><a href="#courses">Courses</a></li>
      <li><a href="/blog">Blog</a></li>
      <li><a href="#contact">Contact</a></li>
      <li><a href="#socials">Socials</a></li>
    </ul>
  </nav>
  <main class="post-container">
    <h1 class="post-title">How to finetune your local llm using LORA</h1>
    <div class="post-meta">
      September 05, 2025
    </div>
    <div class="post-content">
      <h1 id="train-in-hours-not-days-using-lora">Train in hours not days using LORA</h1>
<h2 id="lora--tiny-add-on-brains-">LoRA = Tiny Add-On Brains üß†</h2>

<p>Instead of training all billion parameters, LoRA adds small trainable matrices.</p>

<p>Think of it as teaching just a few neurons, not rewiring the whole brain.</p>

<h2 id="magic-settings-">Magic Settings ‚ö°</h2>

<ul>
  <li>
    <p>Rank (r): How many ‚Äúextra neurons‚Äù LoRA adds (small r = more compression).</p>
  </li>
  <li>
    <p>Alpha: How strong those neurons influence the model.</p>
  </li>
  <li>
    <p>Dropout: Prevents overfitting ‚Äî like making the neurons forget some details.</p>
  </li>
</ul>

<h2 id="result--cheap--fast-training-">Result = Cheap + Fast Training üí∏</h2>

<ul>
  <li>
    <p>Train a model on your laptop in hours (not days).</p>
  </li>
  <li>
    <p>Save GPU memory while keeping accuracy.</p>
  </li>
  <li>
    <p>Perfect for fine-tuning chatbots, code assistants, or small domain models.</p>
  </li>
</ul>

<details>
<summary>Click to expand and see dependancy for this projectüìå</summary>

``` 
name: lora
channels:
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - _openmp_mutex=5.1=1_gnu
  - bzip2=1.0.8=h5eee18b_6
  - ca-certificates=2025.7.15=h06a4308_0
  - expat=2.7.1=h6a678d5_0
  - ld_impl_linux-64=2.40=h12ee557_0
  - libffi=3.4.4=h6a678d5_1
  - libgcc-ng=11.2.0=h1234567_1
  - libgomp=11.2.0=h1234567_1
  - libstdcxx-ng=11.2.0=h1234567_1
  - libuuid=1.41.5=h5eee18b_0
  - libxcb=1.17.0=h9b100fa_0
  - ncurses=6.5=h7934f7d_0
  - openssl=3.0.17=h5eee18b_0
  - pip=25.2=pyhc872135_0
  - pthread-stubs=0.3=h0ce48e5_1
  - python=3.10.18=h1a3bd86_0
  - readline=8.3=hc2a1206_0
  - setuptools=78.1.1=py310h06a4308_0
  - sqlite=3.50.2=hb25bd0a_1
  - tk=8.6.15=h54e0aa7_0
  - wheel=0.45.1=py310h06a4308_0
  - xorg-libx11=1.8.12=h9b100fa_1
  - xorg-libxau=1.0.12=h9b100fa_0
  - xorg-libxdmcp=1.1.5=h9b100fa_0
  - xorg-xorgproto=2024.1=h5eee18b_1
  - xz=5.6.4=h5eee18b_1
  - zlib=1.2.13=h5eee18b_1
  - pip:
      - accelerate==0.31.0
      - aiohappyeyeballs==2.6.1
      - aiohttp==3.12.15
      - aiosignal==1.4.0
      - alembic==1.16.5
      - annotated-types==0.7.0
      - annoy==1.17.3
      - anyio==4.10.0
      - argon2-cffi==25.1.0
      - argon2-cffi-bindings==25.1.0
      - arrow==1.3.0
      - asttokens==3.0.0
      - async-lru==2.0.5
      - async-timeout==4.0.3
      - attrs==25.3.0
      - babel==2.17.0
      - banal==1.0.6
      - beautifulsoup4==4.13.5
      - bertopic==0.16.3
      - bitsandbytes==0.43.1
      - bleach==6.2.0
      - boto3==1.40.24
      - botocore==1.40.24
      - certifi==2025.8.3
      - cffi==1.17.1
      - charset-normalizer==3.4.3
      - click==8.2.1
      - cohere==5.5.8
      - colorcet==3.1.0
      - colorspacious==1.1.2
      - comm==0.2.3
      - contourpy==1.3.2
      - cycler==0.12.1
      - dataclasses-json==0.6.7
      - datamapplot==0.3.0
      - dataset==1.6.2
      - datasets==2.20.0
      - datashader==0.18.2
      - debugpy==1.8.16
      - decorator==5.2.1
      - defusedxml==0.7.1
      - dill==0.3.8
      - diskcache==5.6.3
      - distro==1.9.0
      - docstring-parser==0.17.0
      - duckduckgo-search==7.1.1
      - eval-type-backport==0.2.2
      - evaluate==0.4.2
      - exceptiongroup==1.3.0
      - executing==2.2.1
      - faiss-cpu==1.8.0
      - fastavro==1.12.0
      - fastjsonschema==2.21.2
      - filelock==3.19.1
      - fonttools==4.59.2
      - fqdn==1.5.1
      - frozenlist==1.7.0
      - fsspec==2024.5.0
      - gensim==4.3.2
      - greenlet==3.2.4
      - h11==0.16.0
      - hdbscan==0.8.40
      - hf-xet==1.1.9
      - httpcore==1.0.9
      - httpx==0.28.1
      - httpx-sse==0.4.1
      - huggingface-hub==0.34.4
      - idna==3.10
      - imageio==2.37.0
      - ipykernel==6.30.1
      - ipython==8.37.0
      - ipywidgets==8.1.3
      - isoduration==20.11.0
      - jedi==0.19.2
      - jinja2==3.1.6
      - jmespath==1.0.1
      - joblib==1.5.2
      - json5==0.12.1
      - jsonpatch==1.33
      - jsonpointer==3.0.0
      - jsonschema==4.25.1
      - jsonschema-specifications==2025.4.1
      - jupyter-client==8.6.3
      - jupyter-core==5.8.1
      - jupyter-events==0.12.0
      - jupyter-lsp==2.3.0
      - jupyter-server==2.17.0
      - jupyter-server-terminals==0.5.3
      - jupyterlab==4.2.2
      - jupyterlab-pygments==0.3.0
      - jupyterlab-server==2.27.3
      - jupyterlab-widgets==3.0.15
      - kiwisolver==1.4.9
      - langchain==0.2.5
      - langchain-community==0.2.5
      - langchain-core==0.2.43
      - langchain-openai==0.1.8
      - langchain-text-splitters==0.2.4
      - langsmith==0.1.147
      - lark==1.2.2
      - lazy-loader==0.4
      - llama-cpp-python==0.2.78
      - llvmlite==0.44.0
      - lxml==6.0.1
      - mako==1.3.10
      - markdown-it-py==4.0.0
      - markupsafe==3.0.2
      - marshmallow==3.26.1
      - matplotlib==3.9.0
      - matplotlib-inline==0.1.7
      - mdurl==0.1.2
      - mistune==3.1.4
      - mpmath==1.3.0
      - mteb==1.12.39
      - multidict==6.6.4
      - multipledispatch==1.0.0
      - multiprocess==0.70.16
      - mypy-extensions==1.1.0
      - narwhals==2.3.0
      - nbclient==0.10.2
      - nbconvert==7.16.6
      - nbformat==5.10.4
      - nest-asyncio==1.6.0
      - networkx==3.4.2
      - nltk==3.8.1
      - notebook-shim==0.2.4
      - numba==0.61.2
      - numexpr==2.10.0
      - numpy==1.26.4
      - nvidia-cublas-cu12==12.1.3.1
      - nvidia-cuda-cupti-cu12==12.1.105
      - nvidia-cuda-nvrtc-cu12==12.1.105
      - nvidia-cuda-runtime-cu12==12.1.105
      - nvidia-cudnn-cu12==8.9.2.26
      - nvidia-cufft-cu12==11.0.2.54
      - nvidia-cufile-cu12==1.13.1.3
      - nvidia-curand-cu12==10.3.2.106
      - nvidia-cusolver-cu12==11.4.5.107
      - nvidia-cusparse-cu12==12.1.0.106
      - nvidia-cusparselt-cu12==0.7.1
      - nvidia-nccl-cu12==2.20.5
      - nvidia-nvjitlink-cu12==12.8.93
      - nvidia-nvtx-cu12==12.1.105
      - openai==1.34.0
      - orjson==3.11.3
      - overrides==7.7.0
      - packaging==24.2
      - pandas==2.2.2
      - pandocfilters==1.5.1
      - param==2.2.1
      - parameterized==0.9.0
      - parso==0.8.5
      - peft==0.11.1
      - pexpect==4.9.0
      - pillow==11.3.0
      - platformdirs==4.4.0
      - plotly==6.3.0
      - polars==1.33.0
      - primp==0.15.0
      - prometheus-client==0.22.1
      - prompt-toolkit==3.0.52
      - propcache==0.3.2
      - psutil==7.0.0
      - ptyprocess==0.7.0
      - pure-eval==0.2.3
      - pyarrow==21.0.0
      - pyarrow-hotfix==0.7
      - pycparser==2.22
      - pyct==0.5.0
      - pydantic==2.11.7
      - pydantic-core==2.33.2
      - pygments==2.19.2
      - pylabeladjust==0.1.13
      - pynndescent==0.5.13
      - pyparsing==3.2.3
      - pyqtree==1.0.0
      - python-dateutil==2.9.0.post0
      - python-json-logger==3.3.0
      - pytrec-eval-terrier==0.5.8
      - pytz==2025.2
      - pyyaml==6.0.2
      - pyzmq==27.0.2
      - referencing==0.36.2
      - regex==2025.9.1
      - requests==2.32.5
      - requests-toolbelt==1.0.0
      - rfc3339-validator==0.1.4
      - rfc3986-validator==0.1.1
      - rfc3987-syntax==1.1.0
      - rich==14.1.0
      - rpds-py==0.27.1
      - s3transfer==0.13.1
      - safetensors==0.6.2
      - scikit-image==0.25.2
      - scikit-learn==1.5.0
      - scipy==1.15.3
      - send2trash==1.8.3
      - sentence-transformers==3.0.1
      - sentencepiece==0.2.0
      - seqeval==1.2.2
      - setfit==1.0.3
      - shtab==1.7.2
      - six==1.17.0
      - smart-open==7.3.0.post1
      - sniffio==1.3.1
      - soupsieve==2.8
      - sqlalchemy==1.4.54
      - stack-data==0.6.3
      - sympy==1.14.0
      - tenacity==8.5.0
      - terminado==0.18.1
      - threadpoolctl==3.6.0
      - tifffile==2025.5.10
      - tiktoken==0.11.0
      - tinycss2==1.4.0
      - tokenizers==0.19.1
      - tomli==2.2.1
      - toolz==1.0.0
      - torch==2.3.1
      - tornado==6.5.2
      - tqdm==4.67.1
      - traitlets==5.14.3
      - transformers==4.41.2
      - triton==2.3.1
      - trl==0.9.4
      - typeguard==4.4.4
      - types-python-dateutil==2.9.0.20250822
      - types-requests==2.32.4.20250809
      - typing-extensions==4.15.0
      - typing-inspect==0.9.0
      - typing-inspection==0.4.1
      - tyro==0.9.31
      - tzdata==2025.2
      - umap-learn==0.5.7
      - uri-template==1.3.0
      - urllib3==2.5.0
      - wcwidth==0.2.13
      - webcolors==24.11.1
      - webencodings==0.5.1
      - websocket-client==1.8.0
      - widgetsnbextension==4.0.14
      - wrapt==1.17.3
      - xarray==2025.6.1
      - xxhash==3.5.0
      - yarl==1.20.1
      - zstandard==0.24.0
prefix: /home/sv/anaconda3/envs/lora

```
</details>

<h2 id="see-code-for-quickly-train-model-using-lora">see code for quickly train model using LORA</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># -*- coding: utf-8 -*-
# Install the requirements in Google Colab
# !pip install transformers datasets trl huggingface_hub
</span>
<span class="c1"># Authenticate to Hugging Face
</span>
<span class="c1"># from huggingface_hub import login
#
# login()
</span>
<span class="c1"># for convenience you can create an environment variable containing your hub token as HF_TOKEN
</span>
<span class="sh">"""</span><span class="s">## 2. Load the dataset</span><span class="sh">"""</span>

<span class="c1"># Load a sample dataset
</span><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># TODO: define your dataset and config using the path and name parameters
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">HuggingFaceTB/smoltalk</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">everyday-conversations</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 2. Show first row
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">data sample</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Import necessary libraries
</span><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="n">trl</span> <span class="kn">import</span> <span class="n">SFTConfig</span><span class="p">,</span> <span class="n">SFTTrainer</span><span class="p">,</span> <span class="n">setup_chat_format</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
    <span class="k">else</span> <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># Load the model and tokenizer
</span><span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">HuggingFaceTB/SmolLM2-135M</span><span class="sh">"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">model_name</span>
<span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Set up the chat format
</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="nf">setup_chat_format</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># Set our name for the finetune to be saved &amp;/ uploaded to
</span><span class="n">finetune_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">SmolLM2-FT-MyDataset</span><span class="sh">"</span>
<span class="n">finetune_tags</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">smol-course</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">module_1</span><span class="sh">"</span><span class="p">]</span>


<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span>

<span class="c1"># TODO: Configure LoRA parameters
# r: rank dimension for LoRA update matrices (smaller = more compression)
</span><span class="n">rank_dimension</span> <span class="o">=</span> <span class="mi">6</span>
<span class="c1"># lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)
</span><span class="n">lora_alpha</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1"># lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)
</span><span class="n">lora_dropout</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="n">peft_config</span> <span class="o">=</span> <span class="nc">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="n">rank_dimension</span><span class="p">,</span>  <span class="c1"># Rank dimension - typically between 4-32
</span>    <span class="n">lora_alpha</span><span class="o">=</span><span class="n">lora_alpha</span><span class="p">,</span>  <span class="c1"># LoRA scaling factor - typically 2x rank
</span>    <span class="n">lora_dropout</span><span class="o">=</span><span class="n">lora_dropout</span><span class="p">,</span>  <span class="c1"># Dropout probability for LoRA layers
</span>    <span class="n">bias</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Bias type for LoRA. the corresponding biases will be updated during training.
</span>    <span class="n">target_modules</span><span class="o">=</span><span class="sh">"</span><span class="s">all-linear</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Which modules to apply LoRA to
</span>    <span class="n">task_type</span><span class="o">=</span><span class="sh">"</span><span class="s">CAUSAL_LM</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Task type for model architecture
</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">Before we can start our training we need to define the hyperparameters (`TrainingArguments`) we want to use.</span><span class="sh">"""</span>

<span class="c1"># Training configuration
# Hyperparameters based on QLoRA paper recommendations
</span><span class="n">args</span> <span class="o">=</span> <span class="nc">SFTConfig</span><span class="p">(</span>
    <span class="c1"># Output settings
</span>    <span class="n">output_dir</span><span class="o">=</span><span class="n">finetune_name</span><span class="p">,</span>  <span class="c1"># Directory to save model checkpoints
</span>    <span class="c1"># Training duration
</span>    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Number of training epochs
</span>    <span class="c1"># Batch size settings
</span>    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Batch size per GPU
</span>    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Accumulate gradients for larger effective batch
</span>    <span class="c1"># Memory optimization
</span>    <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># Trade compute for memory savings
</span>    <span class="c1"># Optimizer settings
</span>    <span class="n">optim</span><span class="o">=</span><span class="sh">"</span><span class="s">adamw_torch_fused</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Use fused AdamW for efficiency
</span>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span>  <span class="c1"># Learning rate (QLoRA paper)
</span>    <span class="n">max_grad_norm</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>  <span class="c1"># Gradient clipping threshold
</span>    <span class="c1"># Learning rate schedule
</span>    <span class="n">warmup_ratio</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>  <span class="c1"># Portion of steps for warmup
</span>    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="sh">"</span><span class="s">constant</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Keep learning rate constant after warmup
</span>    <span class="c1"># Logging and saving
</span>    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># Log metrics every N steps
</span>    <span class="n">save_strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">epoch</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Save checkpoint every epoch
</span>    <span class="c1"># Precision settings
</span>    <span class="n">bf16</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># Use bfloat16 precision
</span>    <span class="c1"># Integration settings
</span>    <span class="n">push_to_hub</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># Don't push to HuggingFace Hub
</span>    <span class="n">report_to</span><span class="o">=</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># Disable external logging
</span><span class="p">)</span>


<span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">1512</span>  <span class="c1"># max sequence length for model and packing of the dataset
</span>
<span class="c1"># Create SFTTrainer with LoRA configuration
</span><span class="n">trainer</span> <span class="o">=</span> <span class="nc">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">peft_config</span><span class="o">=</span><span class="n">peft_config</span><span class="p">,</span>  <span class="c1"># LoRA configuration
</span>    <span class="n">max_seq_length</span><span class="o">=</span><span class="n">max_seq_length</span><span class="p">,</span>  <span class="c1"># Maximum sequence length
</span>    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">packing</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># Enable input packing for efficiency
</span>    <span class="n">dataset_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">add_special_tokens</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>  <span class="c1"># Special tokens handled by template
</span>        <span class="sh">"</span><span class="s">append_concat_token</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>  <span class="c1"># No additional separator needed
</span>    <span class="p">},</span>
<span class="p">)</span>

<span class="sh">"""</span><span class="s">Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.</span><span class="sh">"""</span>

<span class="c1"># start training, the model will be automatically saved to the hub and the output directory
</span><span class="n">trainer</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>

<span class="c1"># save model
</span><span class="n">trainer</span><span class="p">.</span><span class="nf">save_model</span><span class="p">()</span>


<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">AutoPeftModelForCausalLM</span>


<span class="c1"># Load PEFT model on CPU
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoPeftModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Merge LoRA and base model and save
</span><span class="n">merged_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">merge_and_unload</span><span class="p">()</span>
<span class="n">merged_model</span><span class="p">.</span><span class="nf">save_pretrained</span><span class="p">(</span>
    <span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_shard_size</span><span class="o">=</span><span class="sh">"</span><span class="s">2GB</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># free the memory again
</span><span class="k">del</span> <span class="n">model</span>
<span class="k">del</span> <span class="n">trainer</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>

</code></pre></div></div>

<h2 id="how-to-use-this-trained-model">How to use this trained model</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">AutoPeftModelForCausalLM</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="n">finetune_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./SmolLM2-FT-MyDataset</span><span class="sh">"</span>

<span class="c1"># 1. Load tokenizer
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">finetune_name</span><span class="p">)</span>

<span class="c1"># 2. Load PEFT (LoRA) model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoPeftModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">finetune_name</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span>
<span class="p">)</span>

<span class="c1"># Optional: merge LoRA weights into base
# model = model.merge_and_unload()
</span>
<span class="c1"># 3. Create inference pipeline
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">text-generation</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># ---- Test inference ----
</span><span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">What is the difference between a fruit and a </span><span class="sh">"</span>
          <span class="sh">"</span><span class="s">vegetable? Give examples of each.</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">generated_text</span><span class="sh">"</span><span class="p">])</span>


</code></pre></div></div>
<p>&lt;/details&gt;</p>

    </div>
  </main>

</body>
</html>
        </section>

        <aside id="sidebar">
          

          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>
        </aside>
      </div>
    </div>

  </body>
</html>
