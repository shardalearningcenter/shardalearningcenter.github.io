<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=" media="screen" type="text/css">
    <link rel="stylesheet" href="/assets/css/print.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How LLM works and summarizes the input | ShardaLearningCenter</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="How LLM works and summarizes the input" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Description Today we learn how the LLM summarize the document. -An encoder–decoder model (also called a sequence-to-sequence model) is a neural network architecture used in natural language processing (NLP) for tasks that transform one sequence into another." />
<meta property="og:description" content="Description Today we learn how the LLM summarize the document. -An encoder–decoder model (also called a sequence-to-sequence model) is a neural network architecture used in natural language processing (NLP) for tasks that transform one sequence into another." />
<link rel="canonical" href="https://shaardalearningcenter.github.io/2025/09/13/how-llm-works-summarizes-the-document.html" />
<meta property="og:url" content="https://shaardalearningcenter.github.io/2025/09/13/how-llm-works-summarizes-the-document.html" />
<meta property="og:site_name" content="ShardaLearningCenter" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-13T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How LLM works and summarizes the input" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-13T00:00:00+05:30","datePublished":"2025-09-13T00:00:00+05:30","description":"Description Today we learn how the LLM summarize the document. -An encoder–decoder model (also called a sequence-to-sequence model) is a neural network architecture used in natural language processing (NLP) for tasks that transform one sequence into another.","headline":"How LLM works and summarizes the input","mainEntityOfPage":{"@type":"WebPage","@id":"https://shaardalearningcenter.github.io/2025/09/13/how-llm-works-summarizes-the-document.html"},"url":"https://shaardalearningcenter.github.io/2025/09/13/how-llm-works-summarizes-the-document.html"}</script>
<!-- End Jekyll SEO tag -->


    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://shaardalearningcenter.github.io/">
          <h1>ShardaLearningCenter</h1>
        </a>
        <h2>Learn to code by building real projects. Fast. Practical. No fluff.</h2>
        
        
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How LLM works and summarizes the input | ShardaLearningCenter</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="/assets/css/style.css">
  <style>
    
    .post-container {
      max-width: 1400px;
      margin: 2rem auto;
      padding: 2rem;
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }
    .post-title {
      font-size: 2.2rem;
      margin-bottom: 0.5rem;
      color: #222;
    }
    .post-meta {
      font-size: 0.9rem;
      color: #666;
      margin-bottom: 1.5rem;
    }
    .post-content {
      font-size: 1.05rem;
      line-height: 1.7;
      color: #333;
    }
    .post-content h2, .post-content h3 {
      margin-top: 1.8rem;
      margin-bottom: 0.8rem;
      font-weight: 600;
      color: #111;
    }
    .post-content p {
      margin-bottom: 1.2rem;
    }
    .post-content a {
      color: #0066cc;
      text-decoration: underline;
    }
    .post-content blockquote {
      border-left: 4px solid #0066cc;
      padding-left: 1rem;
      font-style: italic;
      color: #555;
      background: #f9f9f9;
      margin: 1.5rem 0;
    }
    .post-content code {
      background: #f4f4f4;
      padding: 0.2rem 0.4rem;
      border-radius: 4px;
      font-size: 0.95rem;
    }
    .post-content pre {
      background: #272822;
      color: #f8f8f2;
      padding: 1rem;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 0.9rem;
    }
  </style>
</head>
<body>
  <nav>
    <h2>Sharda Learning Center</h2>
    <ul>
      <li><a href="#home">Home</a></li>
      <li><a href="#about">About</a></li>
      <li><a href="#courses">Courses</a></li>
      <li><a href="/blog">Blog</a></li>
      <li><a href="#contact">Contact</a></li>
      <li><a href="#socials">Socials</a></li>
    </ul>
  </nav>
  <main class="post-container">
    <h1 class="post-title">How LLM works and summarizes the input</h1>
    <div class="post-meta">
      September 13, 2025
    </div>
    <div class="post-content">
      <h1 id="description">Description</h1>
<h2 id="today-we-learn-how-the-llm-summarize-the-document">Today we learn how the LLM summarize the document.</h2>
<p>-An encoder–decoder model (also called a sequence-to-sequence model) is a neural network architecture used in natural language processing (NLP) for tasks that transform one sequence into another.</p>

<p>-The <strong>encoder</strong> reads the <strong>input sequence</strong> (e.g., a sentence) and converts it into a rich contextual representation.</p>

<p>-The <strong>decoder</strong> takes this representation and generates the <strong>output sequence</strong> step by step (e.g., a translated sentence or completed text).</p>

<p>-This architecture powers models like T5, FLAN-T5, and BART, making them suitable for tasks such as translation, summarization, question answering, and sentence completion. Unlike GPT (decoder-only) or BERT (encoder-only), encoder–decoder models combine both reading (understanding) and writing (generating).</p>
<h2 id="tokenizer">Tokenizer</h2>
<ul>
  <li>Convert human input to number presentation
    <h2 id="seq2seq">Seq2Seq</h2>
  </li>
  <li>Generate take input sequence generate output sequence
    <h1 id="here-is-code">Here is code</h1>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load model + tokenizer
model_name = "google/flan-t5-base"   # T5-base, instruction-tuned
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Example 1: Summarization
text = "summarize: The sun rises in the east and sets in the west. It is an important fact in geography."
inputs = tokenizer(text, return_tensors="pt")
outputs = model.generate(**inputs, max_length=40)
print("Summarization:", tokenizer.decode(outputs[0], skip_special_tokens=True))

</code></pre></div></div>

    </div>
  </main>

</body>
</html>
        </section>

        <aside id="sidebar">
          

          

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</p>
        </aside>
      </div>
    </div>

  </body>
</html>
